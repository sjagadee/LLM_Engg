{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d36c969",
   "metadata": {},
   "source": [
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,\n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b919816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, update_display\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efcd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4.1-mini'\n",
    "MODEL_OLLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b557acd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b260b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutor_system_prompt = \"\"\"\n",
    "You are a techinical tutor assistant.\n",
    "You will help students with their coding problems, and you will provide a step by step explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd80766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt():\n",
    "    prompt = \"\"\"\n",
    "Here is a code snippet that I would like you to explain:\n",
    "\n",
    "def fetch_website_links(url):\n",
    "    '''\n",
    "    Return the links on the webiste at the given url\n",
    "    I realize this is inefficient as we're parsing twice! This is to keep the code in the lab simple.\n",
    "    Feel free to use a class and optimize it!\n",
    "    '''\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "    return [link for link in links if link]\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a57117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_code_openai():\n",
    "    openai = OpenAI()\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": tutor_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_user_prompt()}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14e4cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's go through the code step-by-step.\n",
       "\n",
       "### Function: `fetch_website_links(url)`\n",
       "\n",
       "#### Purpose:\n",
       "This function takes a URL as input and returns a list of all the hyperlinks (`href` attributes) found on the web page at that URL.\n",
       "\n",
       "---\n",
       "\n",
       "### Step-by-step explanation:\n",
       "\n",
       "1. **Function Definition and Docstring:**\n",
       "   ```python\n",
       "   def fetch_website_links(url):\n",
       "       '''\n",
       "       Return the links on the website at the given url\n",
       "       I realize this is inefficient as we're parsing twice! This is to keep the code in the lab simple.\n",
       "       Feel free to use a class and optimize it!\n",
       "       '''\n",
       "   ```\n",
       "   - The function `fetch_website_links` expects one parameter called `url`.\n",
       "   - The docstring explains that it returns all the links on the website at the specified URL.\n",
       "   - It also mentions there's an inefficiency due to parsing the content twice (though in this snippet it's not explicitly shown), and hints at an optimization with classes if desired.\n",
       "\n",
       "2. **Making an HTTP GET Request:**\n",
       "   ```python\n",
       "   response = requests.get(url, headers=headers)\n",
       "   ```\n",
       "   - This line sends an HTTP GET request to the given `url`.\n",
       "   - The `headers` argument is provided, likely to mimic a browser or include necessary headers (user-agent, etc.), but `headers` must be defined somewhere outside this snippet.\n",
       "   - `requests.get` returns a `response` object containing the web page data.\n",
       "\n",
       "3. **Parsing the HTML Content:**\n",
       "   ```python\n",
       "   soup = BeautifulSoup(response.content, \"html.parser\")\n",
       "   ```\n",
       "   - `response.content` contains the raw bytes of the webpage fetched.\n",
       "   - `BeautifulSoup` is a popular Python library for parsing HTML and XML documents.\n",
       "   - Here, the content is parsed using the `\"html.parser\"` which is Python's built-in HTML parser.\n",
       "   - `soup` now contains a parsed tree of the HTML content.\n",
       "\n",
       "4. **Extracting all `<a>` Tags:**\n",
       "   ```python\n",
       "   links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
       "   ```\n",
       "   - `soup.find_all(\"a\")` finds all anchor tags (`<a>`) in the HTML.\n",
       "   - The list comprehension iterates over each anchor tag and gets the value of its `href` attribute (the actual link).\n",
       "   - Some `<a>` tags may not have an `href` attribute, so `.get(\"href\")` returns `None` in those cases.\n",
       "   - `links` is a list of strings (the link URLs) or `None` values.\n",
       "\n",
       "5. **Filtering out `None` Values:**\n",
       "   ```python\n",
       "   return [link for link in links if link]\n",
       "   ```\n",
       "   - This filters the `links` list to exclude any that are `None` or empty. Only valid strings remain.\n",
       "   - The resulting list is returned.\n",
       "\n",
       "---\n",
       "\n",
       "### Summary\n",
       "\n",
       "- The function fetches the webpage at a given URL.\n",
       "- Parses the HTML to find all anchor tags `<a>`.\n",
       "- Extracts the URLs from the `href` attributes.\n",
       "- Returns a clean list of valid links.\n",
       "\n",
       "---\n",
       "\n",
       "### Notes:\n",
       "- **Efficiency and Optimization:**  \n",
       "  The comment about inefficiency likely refers to cases where multiple parsings or multiple requests happen, but this snippet parses once.\n",
       "- **Prerequisites:**  \n",
       "  You need to import the relevant libraries and define `headers`, for example:\n",
       "\n",
       "  ```python\n",
       "  import requests\n",
       "  from bs4 import BeautifulSoup\n",
       "\n",
       "  headers = {\n",
       "      \"User-Agent\": \"Mozilla/5.0 ...\"\n",
       "  }\n",
       "  ```\n",
       "\n",
       "If you want, I can also help you rewrite or optimize this code!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain_code_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f20e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6fa6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_code_ollama():\n",
    "    # Setting up ollama to use it using the OpenAI API\n",
    "    OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "    ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=MODEL_OLLAMA,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": tutor_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_user_prompt()}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749c54cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down this code snippet and discuss its efficiency.\n",
       "\n",
       "**Functionality**\n",
       "\n",
       "The `fetch_website_links` function takes a URL as input, retrieves the webpage content using `requests`, parses the HTML content using BeautifulSoup, extracts all the anchor tags (`<a>`) from the parsed content, and returns a list of link URLs associated with these anchor tags.\n",
       "\n",
       "**Code Review**\n",
       "\n",
       "Here's a step-by-step explanation:\n",
       "\n",
       "1. `response = requests.get(url, headers=headers)`: This line sends an HTTP GET request to the specified URL and stores the response in the `response` variable.\n",
       "2. `soup = BeautifulSoup(response.content, \"html.parser\")`: This line parses the HTML content of the response using BeautifulSoup and creates a parse tree (i.e., a representation of the structure of the HTML document).\n",
       "3. `links = [link.get(\"href\") for link in soup.find_all(\"a\")]`: This list comprehension extracts all anchor tags (`<a>`) from the parsed content and retrieves their href attributes, which contain the link URLs.\n",
       "4. `return [link for link in links if link]`: This line filters out any empty strings or None values from the extracted link URLs (i.e., removes unnecessary entries) and returns the filtered list.\n",
       "\n",
       "**Inefficiency**\n",
       "\n",
       "As you mentioned, this approach is inefficient because it involves parsing the HTML content twice:\n",
       "\t* Once using BeautifulSoup.\n",
       "\t* Again when extracting anchor tags (`<a>`) and their href attributes.\n",
       "\n",
       "A more efficient approach would involve combining these two parse operations into a single pass over the parsed HTML tree. We can take advantage of BeautifulSoup's `find_all` method to extract all link URLs directly in one go, without needing to repeat the parsing step.\n",
       "\n",
       "**Optimized Version**\n",
       "\n",
       "Here's an optimized version of the function that uses BeautifulSoup's `extract` method with a CSS selector:\n",
       "```python\n",
       "def fetch_website_links(url):\n",
       "    response = requests.get(url, headers=headers)\n",
       "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
       "    links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]  # extract links with href attribute\n",
       "    return [link for link in links if link]\n",
       "```\n",
       "However, this approach still repeats the parsing step. To truly optimize the function, we can create a BeautifulSoup object and use its `text_content` method to parse the HTML content into a string, which can then be processed without repeating the parsing step.\n",
       "```python\n",
       "from bs4 import BeautifulSoup\n",
       "\n",
       "def fetch_website_links(url):\n",
       "    response = requests.get(url, headers=headers)\n",
       "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
       "    \n",
       "    def extract_links(html_tree):\n",
       "        \"\"\"Helper function to extract links from an HTML tree\"\"\"\n",
       "        return [a.get(\"href\") for a in html_tree.find_all(\"a\", href=True)]\n",
       "\n",
       "    # Create a Soup object to avoid repeating parsing\n",
       "    s = soup.decode_contents()  # decode contents to process the parsed content\n",
       "    \n",
       "    # Process the parsed content without re-parsing it\n",
       "    links = extract_links(s)\n",
       "    \n",
       "    return [link for link in links if link]\n",
       "```\n",
       "This optimized version of the function should be much more efficient than the original code snippet.\n",
       "\n",
       "**Additional Advice**\n",
       "\n",
       "* Always use try-except blocks to handle exceptions that may occur during request or parsing operations.\n",
       "* Consider using a more modern Python version (e.g., Python 3.8+).\n",
       "* Familiarize yourself with common Python libraries and their methods for web scraping, such as `requests`, `BeautifulSoup`, `lxml` (for HTML parsing), et al.\n",
       "\n",
       "Do you have any specific questions or areas of the code that you'd like me to expand upon?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain_code_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729dce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
