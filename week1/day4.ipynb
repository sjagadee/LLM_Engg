{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1960433a",
   "metadata": {},
   "source": [
    "### Tokenizing with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e042f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12194, 922, 1308, 382, 36013, 2363, 0]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4.1-mini\")\n",
    "\n",
    "tokens = encoding.encode(\"Hi my name is Srini!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38936df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12194: Hi\n",
      "922:  my\n",
      "1308:  name\n",
      "382:  is\n",
      "36013:  Sr\n",
      "2363: ini\n",
      "0: !\n"
     ]
    }
   ],
   "source": [
    "# lets decode \n",
    "for token in tokens:\n",
    "    print(f\"{token}: {encoding.decode([token])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112f0d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the\n",
      " are\n",
      "$\n"
     ]
    }
   ],
   "source": [
    "print(encoding.decode([290]))\n",
    "print(encoding.decode([553]))\n",
    "print(encoding.decode([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511aedd",
   "metadata": {},
   "source": [
    "### Illusion of \"memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738c3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01e257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "935de872",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b12f8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant'},\n",
    "    {'role': 'user', 'content': 'Who won the world series in 2020?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Los Angeles Dodgers won the World Series in 2020. They defeated the Tampa Bay Rays to win the championship.\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d696596",
   "metadata": {},
   "source": [
    "Ask a follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a75971d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant'},\n",
    "    {'role': 'user', 'content': 'Who won in 2021?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you please specify which event or competition you are referring to for the year 2021?\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4d456",
   "metadata": {},
   "source": [
    "We just mentioned about a \"world series\" in the previous API call, it could not remember it in the next call. Why ??\n",
    "\n",
    "Because all API calls to an LLM are stateless, that is it does not prior information about the calls we make. Tasks we asked to perform.\n",
    "\n",
    "As `AI Engineers` it is our responsibility to device a technique to give an impression to the user that LLM has \"memory\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a87ac4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant'},\n",
    "    {'role': 'user', 'content': 'Who won the world series in 2020?'},\n",
    "    {'role': 'assistant', 'content': 'The Los Angeles Dodgers won the World Series in 2020.'},\n",
    "    {'role': 'user', 'content': 'Who won in 2021?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29092554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Atlanta Braves won the World Series in 2021.\n"
     ]
    }
   ],
   "source": [
    "response =openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a153197",
   "metadata": {},
   "source": [
    "### To recap:\n",
    "\n",
    "- Every call to an LLM is stateless\n",
    "- We pass in the entire conversation so far in the input prompt, every time\n",
    "- This gives the illusion that the LLM has memory - it apparently keeps the context of the conversation \n",
    "- But this is a trick; it's a by-product of providing the entire conversation, every time an LLM just predicts the most likely next tokens in the sequence; if that sequence contains \"My name is Srini\" and later \"What's my name?\" then it will predict.. Srini!\n",
    "- The ChatGPT product uses exactly this trick - every time you send a message, it's the entire conversation that gets passed in.\n",
    "\n",
    "\"Does that mean we have to pay extra each time for all the conversation so far\"\n",
    "\n",
    "For sure it does. And that's what we WANT. We want the LLM to predict the next tokens in the sequence, looking back on the entire conversation. We want that compute to happen, so we need to pay the electricity bill for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c9bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
