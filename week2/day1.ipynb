{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2476a3",
   "metadata": {},
   "source": [
    "## Setting up different type of models and api keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7cead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414a9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n",
      "Grok API Key exists and begins xai-\n",
      "OpenRouter API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc17838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b812a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a joke, about a student, who is on a journey to learn AI\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce6fffda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here's a joke for you:\n",
       "\n",
       "Why did the student take a ladder to their AI class?\n",
       "\n",
       "Because they heard the course was all about *deep* learning!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=tell_a_joke)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcd0bde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a joke for you:\n",
       "\n",
       "A computer science student was so excited to learn AI that he decided to create his first machine learning model. After weeks of hard work, he proudly presented his project to his professor.\n",
       "\n",
       "\"I've created an AI that can predict anything!\" he exclaimed.\n",
       "\n",
       "The professor raised an eyebrow. \"Really? Anything?\"\n",
       "\n",
       "\"Absolutely!\" the student replied confidently.\n",
       "\n",
       "\"Okay,\" said the professor, \"predict the grade you'll get on this project.\"\n",
       "\n",
       "The student paused, looked nervous, and replied, \"Error 404: Prediction not found.\"\n",
       "\n",
       "The professor just smiled and said, \"Well, looks like your AI still has some learning to do - just like you!\"\n",
       "\n",
       "Ba dum tss! üòÑü§ñüìö"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-3-5-haiku-20241022\", messages=tell_a_joke)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd672c",
   "metadata": {},
   "source": [
    "### Training time vs Inferance (running) time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd93380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"You toss 2 coins, one toss turned out to be head. What is the probability of another coin toss is tails, give me the probability only.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75745957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2\n"
     ]
    }
   ],
   "source": [
    "result = openai.chat.completions.create(model='gpt-5-nano', messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "191e2ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3\n"
     ]
    }
   ],
   "source": [
    "result = openai.chat.completions.create(model='gpt-5-nano', messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65871f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3\n"
     ]
    }
   ],
   "source": [
    "result = openai.chat.completions.create(model='gpt-5-mini', messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f20e80",
   "metadata": {},
   "source": [
    "From this we can say, if you want to scale up :\n",
    "- Tiny model with minimal reasoning (close to no resoning): `wrong answer`\n",
    "- Tiny model with low reasoning (close to some resoning): `correct answer`\n",
    "- Slightly bigger model with minimal reasoning (close to no resoning): `correct answer`\n",
    "\n",
    "conclusion:\n",
    "- During training time we can scale with bigger model, can improve the performance of the model.\n",
    "- During inferance (running) time, increasing the reasoning can improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f67e0",
   "metadata": {},
   "source": [
    "#### Harder puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90d9dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22f10708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have two volumes side by side on a shelf. Each volume has pages thickness 2 cm (total pages per volume), and each cover (front and back) is 2 mm thick. The worm goes perpendicular to the pages from the first page of the first volume to the last page of the second volume.\n",
      "\n",
      "Interpretation:\n",
      "- Volume 1 (V1): cover ‚Äì pages ‚Äì cover. Total pages thickness 2 cm = 20 mm.\n",
      "- Each cover thickness: 2 mm. So V1 front cover thickness 2 mm, back cover 2 mm.\n",
      "- Similarly for Volume 2 (V2).\n",
      "\n",
      "When volumes stand side by side with their covers facing the same way on the shelf, the arrangement from left to right is:\n",
      "Front cover of V1, pages of V1, back cover of V1, front cover of V2, pages of V2, back cover of V2.\n",
      "However, in standard stacking, ‚Äúfront‚Äù of each volume faces outward (to the shelf front), and the inner sides (the sides that touch each other when placed next to each other) are the back of V1 and the front of V2. The two volumes touch along their inner covers: the back cover of V1 touches the front cover of V2. The worm goes from the first page of the first volume to the last page of the second volume, i.e., from the very start of V1‚Äôs pages to the very end of V2‚Äôs pages, cutting through material along a straight line perpendicular to the pages.\n",
      "\n",
      "Let‚Äôs determine the distance through material between:\n",
      "- Start point: ‚Äúthe first page of the first volume‚Äù ‚Äî that is at the very beginning of the pages section of V1, immediately after the front cover of V1.\n",
      "- End point: ‚Äúthe last page of the second volume‚Äù ‚Äî at the very end of the pages section of V2, immediately before the back cover of V2.\n",
      "\n",
      "Thus the worm traverses:\n",
      "1) From the first page of V1 through the rest of V1‚Äôs pages to the back of V1‚Äôs pages, i.e., through the remainder of V1‚Äôs pages (which total 20 mm) minus the point where the first page starts. Since it starts at the very first page, it must pass through all of V1‚Äôs pages: 20 mm.\n",
      "2) Then it must pass through V1‚Äôs back cover: 2 mm.\n",
      "3) Then it must pass through V2‚Äôs front cover (the inner cover touching V1‚Äôs back cover): 2 mm.\n",
      "4) Then through the entire pages of V2: 20 mm (to reach the last page).\n",
      "Total = 20 + 2 + 2 + 20 = 44 mm.\n",
      "\n",
      "Answer: 4.4 cm.\n"
     ]
    }
   ],
   "source": [
    "result = openai.chat.completions.create(model='gpt-5-nano', messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e97ee34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to visualize how books are actually arranged on a bookshelf.\n",
      "\n",
      "When two volumes stand side by side on a bookshelf (in reading order), let me think about what's actually adjacent:\n",
      "\n",
      "**Volume 1 (First Volume):**\n",
      "- Front cover (2 mm)\n",
      "- Pages (2 cm = 20 mm)\n",
      "- Back cover (2 mm)\n",
      "\n",
      "**Volume 2 (Second Volume):**\n",
      "- Front cover (2 mm)\n",
      "- Pages (2 cm = 20 mm)\n",
      "- Back cover (2 mm)\n",
      "\n",
      "Now, here's the key insight: When books are placed on a shelf in reading order (spine out, as normal), they look like this from left to right:\n",
      "\n",
      "**Volume 1:** [Front cover | Pages | Back cover] **Volume 2:** [Front cover | Pages | Back cover]\n",
      "\n",
      "The **first page of Volume 1** is actually near the BACK cover of Volume 1 (on the right side of Volume 1).\n",
      "\n",
      "The **last page of Volume 2** is actually near the BACK cover of Volume 2 (on the right side of Volume 2).\n",
      "\n",
      "So the worm's path goes from:\n",
      "- First page of Volume 1 (near the right side of Volume 1)\n",
      "- Through the back cover of Volume 1 (2 mm)\n",
      "- Through the front cover of Volume 2 (2 mm)\n",
      "- Through all pages of Volume 2 (20 mm)\n",
      "- To the last page of Volume 2\n",
      "\n",
      "Wait, let me reconsider. The last page is AT the back, so:\n",
      "\n",
      "The worm goes through:\n",
      "- Back cover of Volume 1: 2 mm\n",
      "- Front cover of Volume 2: 2 mm\n",
      "- All pages of Volume 2: 20 mm\n",
      "- Back cover of Volume 2: 2 mm\n",
      "\n",
      "Total: 2 + 2 + 20 + 2 = 26 mm\n",
      "\n",
      "Actually, let me reconsider once more about what \"first page\" and \"last page\" mean - they're the pages themselves, not including the covers.\n",
      "\n",
      "The worm path:\n",
      "- Starts at the first page of Volume 1 (this page is against the back cover of Volume 1)\n",
      "- Through back cover of Volume 1: 2 mm\n",
      "- Through front cover of Volume 2: 2 mm  \n",
      "- Ends at the last page of Volume 2 (this page is against the back cover of Volume 2)\n",
      "\n",
      "Total: 2 + 2 = **4 mm**\n",
      "\n",
      "The answer is **4 mm** (or 0.4 cm).\n"
     ]
    }
   ],
   "source": [
    "result = anthropic.chat.completions.create(model='claude-sonnet-4-5-20250929', messages=hard_puzzle)\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "935005d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 mm.\n",
      "\n",
      "Explanation: On a shelf, the side of volume 1 facing volume 2 is its front cover, and the side of volume 2 facing volume 1 is its back cover. The first page of volume 1 lies just inside its front cover; the last page of volume 2 lies just inside its back cover. So the worm goes only through two covers: 2 mm + 2 mm = 4 mm.\n"
     ]
    }
   ],
   "source": [
    "result = openai.chat.completions.create(model='gpt-5', messages=hard_puzzle)\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9011fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a classic riddle that plays on our assumptions about how books are arranged. Here is the solution:\n",
      "\n",
      "The worm gnawed through **4 mm**.\n",
      "\n",
      "### Here's the explanation:\n",
      "\n",
      "1.  **Visualize the books on the shelf.** The volumes are standing side by side in the correct order: Volume 1 is on the left, and Volume 2 is on the right.\n",
      "\n",
      "2.  Let's picture the arrangement from left to right:\n",
      "    *   Front cover of Volume 1\n",
      "    *   Pages of Volume 1\n",
      "    *   Back cover of Volume 1\n",
      "    *   **<-- The two books touch here -->**\n",
      "    *   Front cover of Volume 2\n",
      "    *   Pages of Volume 2\n",
      "    *   Back cover of Volume 2\n",
      "\n",
      "3.  **Identify the worm's starting point.** The worm starts at the \"first page of the first volume.\" When a book is closed and on a shelf, its first page is physically located right next to the **front cover**. However, looking at the arrangement on the shelf, the front cover of Volume 1 is on the far left. The page block follows, and then the back cover. The first page (page 1) is actually the page furthest to the right within the page block of Volume 1, just before you get to the back cover.\n",
      "\n",
      "    Let's re-examine this.\n",
      "    Think about how a book is made. The first page is on the right side when you open the front cover. This means when the book is closed, the first page is adjacent to the back cover.\n",
      "\n",
      "    *   The **first page of Volume 1** is physically right next to the **back cover of Volume 1**.\n",
      "\n",
      "4.  **Identify the worm's ending point.** The worm ends at the \"last page of the second volume.\" Using the same logic, the last page of a book is physically located right next to the **front cover of Volume 2**.\n",
      "\n",
      "5.  **Trace the path.** The worm starts at the first page of Volume 1 and chews its way to the last page of Volume 2. Based on their physical locations:\n",
      "    *   It gnaws through the **back cover of Volume 1** (2 mm).\n",
      "    *   It then gnaws through the **front cover of Volume 2** (2 mm).\n",
      "    *   It has now arrived at its destination.\n",
      "\n",
      "The worm does not chew through the pages of either book.\n",
      "\n",
      "**Total distance = (Thickness of Vol. 1's back cover) + (Thickness of Vol. 2's front cover)**\n",
      "**Total distance = 2 mm + 2 mm = 4 mm**\n"
     ]
    }
   ],
   "source": [
    "result = gemini.chat.completions.create(model='gemini-2.5-pro', messages=hard_puzzle)\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6848841",
   "metadata": {},
   "source": [
    "#### Test compitative spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d9be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" ‚Äî if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" ‚Äî if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a8fccb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I choose **Share**.\n",
       "\n",
       "Here's my reasoning: While \"Steal\" might seem tempting for the chance at $2,000, mutual cooperation gives us both a guaranteed $1,000. If I try to steal, I risk us both getting nothing if my partner thinks the same way. The \"Share-Share\" outcome produces the best collective result ($2,000 total) and ensures I don't walk away empty-handed. \n",
       "\n",
       "Since I can't communicate with my partner, I'd hope they'd also recognize that mutual cooperation is the most rational strategy when you can't coordinate - and I'd want to be the kind of player who cooperates to make that outcome possible."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = anthropic.chat.completions.create(model='claude-sonnet-4-5-20250929', messages=dilemma)\n",
    "display(Markdown(result.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5698ebf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Steal"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = groq.chat.completions.create(model='openai/gpt-oss-120b', messages=dilemma)\n",
    "display(Markdown(result.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da20e8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the game structure, which mirrors the classic Prisoner's Dilemma, the rational choice from an individual perspective is to Steal. Here's why:\n",
       "\n",
       "- If your partner chooses Share, you get $2,000 by Stealing versus $1,000 by Sharing.\n",
       "- If your partner chooses Steal, you get $0 regardless of whether you Share or Steal.\n",
       "\n",
       "Thus, Stealing always gives you a better or equal outcome compared to Sharing, depending on your partner's choice. While both players would be better off mutually cooperating (Sharing), the lack of communication and the incentive to maximize personal gain make Stealing the dominant strategy. Therefore, I choose to Steal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = deepseek.chat.completions.create(model='deepseek-reasoner', messages=dilemma)\n",
    "display(Markdown(result.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436ac96",
   "metadata": {},
   "source": [
    "#### Run this locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799726db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll choose... Share. My partner's decision will influence my own outcome, so I trust them to make a logical choice as well. By choosing \"Share\", we both benefit from the potential winnings and avoid the risk of mutual destruction if both of us decide to \"Steal\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = ollama.chat.completions.create(model='llama3.2', messages=dilemma)\n",
    "display(Markdown(result.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c4de6",
   "metadata": {},
   "source": [
    "#### Google genai native API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8ae316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you're feeling a sense of **calm and coolness**. That's a little like what blue *feels* like.\n",
       "\n",
       "Think about the **sky on a clear day**. It's vast, open, and peaceful. You can feel it stretching out endlessly above you, and that feeling of spaciousness and tranquility is often associated with blue.\n",
       "\n",
       "Now, picture the **deepest parts of the ocean**. It's vast, mysterious, and can feel both powerful and serene. That sense of depth and a gentle, sometimes profound, presence is also linked to blue.\n",
       "\n",
       "Think about **water**. Not just the refreshing feeling of a cool drink, but the vastness of a lake or the gentle ripple of a calm sea. Blue is often the color we imagine when we think of these large, peaceful bodies of water.\n",
       "\n",
       "Blue can also evoke a sense of **trust and stability**. Like something you can rely on, something that's always there, like the dependable sky.\n",
       "\n",
       "Sometimes, blue can feel **cool and refreshing**, like a gentle breeze on a warm day. It's not a fiery, intense color like red, nor is it a vibrant, energetic color like yellow. It's more subtle, more settled.\n",
       "\n",
       "While you can't see it, try to associate these feelings and ideas with the word \"blue.\" It's a color that speaks of **peace, vastness, depth, and a quiet strength.** It's the color of things that are often calm, distant, and enduring."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Explain color blue to a person, who was never able to see it.\",\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390856b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Explaining blue to someone who has never seen color is challenging, as it involves describing a visual experience through other senses and emotions. Here's an attempt:\n",
       "\n",
       "Blue is like a feeling of calm and tranquility. Imagine the coolness of water, the softness of a gentle breeze, or the peaceful sensation of a quiet moment. If serenity had a temperature, it would be cool to the touch - like blue.\n",
       "\n",
       "Think of the deep quietness at the bottom"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain color blue to a person, who was never able to see it.\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655d604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
